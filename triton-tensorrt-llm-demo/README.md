# Triton + TensorRT LLM Inference Demo

## ðŸ“˜ Overview
This project demonstrates deployment and optimization of a **Large Language Model (LLM)** using **NVIDIA Triton Inference Server** and **TensorRT-LLM**.  
It benchmarks token generation speed and latency before and after TensorRT optimization.

## ðŸŽ¯ Objectives
- Convert a PyTorch model (e.g., Llama 3 or Mistral) to ONNX format
- Serve via Triton Inference Server
- Optimize with TensorRT-LLM
- Compare performance metrics and GPU utilization

## ðŸ§± Architecture
